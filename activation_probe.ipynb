{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db17f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded Qwen/Qwen2.5-3B-Instruct on cuda with 36 transformer layers.\n",
      "[INFO] Saved activations for prompt 1\n",
      "[INFO] Saved activations for prompt 2\n",
      "[INFO] Saved activations for prompt 3\n",
      "[INFO] Saved activations for prompt 4\n",
      "[INFO] Saved activations for prompt 5\n",
      "[INFO] Activation extraction complete → activations/\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "activation_probe.py — Phase 3.1–3.2 Activation Extraction\n",
    "Captures hidden activations from all transformer layers of Qwen2.5-3B-Instruct\n",
    "and saves mean-pooled activations as NumPy tensors (safe hook version).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Model Loading\n",
    "# ---------------------------------------------------------------------\n",
    "def load_model(model_name=\"Qwen/Qwen2.5-3B-Instruct\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "    n_layers = len(model.model.layers)\n",
    "    print(f\"[INFO] Loaded {model_name} on {device} with {n_layers} transformer layers.\")\n",
    "    return model, tokenizer, device, n_layers\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Hook registration (safe version)\n",
    "# ---------------------------------------------------------------------\n",
    "def register_hooks(model, store):\n",
    "    \"\"\"Attach forward hooks that copy activations without modifying the graph.\"\"\"\n",
    "    handles = []\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        def hook_fn(module, inp, out, layer_idx=idx):\n",
    "            # out[0] is the hidden state tensor, clone & move later\n",
    "            store[layer_idx] = out[0].detach().cpu()\n",
    "        handles.append(layer.register_forward_hook(hook_fn))\n",
    "    return handles\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Activation Capture\n",
    "# ---------------------------------------------------------------------\n",
    "def capture_activations(model, tokenizer, device, prompts, save_dir=\"activations\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    store = {}\n",
    "\n",
    "    hooks = register_hooks(model, store)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, prompt in enumerate(prompts, start=1):\n",
    "            store.clear()\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            _ = model(**inputs)\n",
    "\n",
    "            # Mean-pool sequence dimension to reduce size\n",
    "            for layer_idx, tensor in store.items():\n",
    "                act = tensor.mean(dim=1).squeeze(0).numpy()  # (hidden_dim,)\n",
    "                np.save(f\"{save_dir}/prompt{i:02d}_layer{layer_idx:02d}.npy\", act)\n",
    "            print(f\"[INFO] Saved activations for prompt {i}\")\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    print(f\"[INFO] Activation extraction complete → {save_dir}/\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. Main entry\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    # Get latest recommender log (for prompt text)\n",
    "    log_dir = \"logs\"\n",
    "    log_files = sorted(\n",
    "        f for f in os.listdir(log_dir)\n",
    "        if f.startswith(\"recommender_\") and f.endswith(\".json\")\n",
    "    )\n",
    "    if not log_files:\n",
    "        raise FileNotFoundError(\"No recommender_*.json log found.\")\n",
    "    latest_log = os.path.join(log_dir, log_files[-1])\n",
    "\n",
    "    with open(latest_log, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    prompts = [r[\"question\"] for r in data[\"records\"]]\n",
    "\n",
    "    model, tokenizer, device, n_layers = load_model()\n",
    "    capture_activations(model, tokenizer, device, prompts)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lighteval_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
