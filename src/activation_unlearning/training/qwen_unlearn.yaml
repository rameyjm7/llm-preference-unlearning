model_name_or_path: Qwen/Qwen2.5-3B-Instruct
stage: sft
finetuning_type: lora

dataset:
  - unlearn

dataset_dir: ./data
output_dir: output/qwen-unlearn

num_train_epochs: 6
max_steps: 30

per_device_train_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 3e-5

lora_rank: 64
lora_alpha: 64
lora_dropout: 0.05

lr_scheduler_type: cosine
warmup_ratio: 0.05
fp16: true

save_steps: 10
logging_steps: 1

report_to: none
log_level: debug
log_level_replica: debug
disable_tqdm: false
overwrite_output_dir: true
packing: false

do_train: true

